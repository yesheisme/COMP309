{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_IwlDjrYx1d"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy.random as rng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMBeODC3gb0B"
      },
      "source": [
        "# let's try something using a small dataset of small images\n",
        "\n",
        "sklearn has [this](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) nice set of 1797 items, each of which is an 8-by-8 grayscale image of a digit. It's a classification task: one tries to predict the target class, 0-9.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.keys()"
      ],
      "metadata": {
        "id": "KWAg_9VOcn8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(digits.data.shape)\n",
        "print(digits.target.shape)"
      ],
      "metadata": {
        "id": "CB8ihomNZTEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = rng.randint(0,1796)   #pick one at random to check\n",
        "plt.imshow(digits.data[i,:].reshape(8,8), cmap='gray')\n",
        "print(i, \" is a \", digits.target[i])"
      ],
      "metadata": {
        "id": "KXftRQ7pEjM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpyZOHEyaBTf"
      },
      "source": [
        "### Convert the input data into `torch` tensors.\n",
        "\n",
        "*Note: I found I had to make sure the torch data tensor was `Float` by, calling `.float()` on it. Apparently numpy default was Float64 (\"Double\") while torch is Float32 (\"Float\").*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H4JV2TfJY7G"
      },
      "source": [
        "print(digits.data.dtype)\n",
        "data = torch.from_numpy(digits.data).float()\n",
        "targ = torch.from_numpy(digits.target).long()\n",
        "print(data.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the same as the above, but with the tensor. Note use of `view`\n"
      ],
      "metadata": {
        "id": "BRihhO3SFozU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CevX6KbLWSZu"
      },
      "source": [
        "plt.gray()\n",
        "i = rng.randint(len(data))\n",
        "plt.matshow(data[i].view(8,8))\n",
        "plt.show()\n",
        "print(i, \" is a \", targ[i].numpy())  # the .numpy is just to stop it saying \"tensor(8)...\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY9yb5aSXF6a"
      },
      "source": [
        "# Let's construct a neural net, with pure torch\n",
        "(My version isn't quite \"pure\" torch in fact - when it comes to defining a **loss**, I'm going to use the `torch.nn` loss function, just because it's handy...)\n",
        "\n",
        "\n",
        "A good structure to adopt is the make the network a `class`, with two methods\n",
        "\n",
        " *   `__init__()` to set up the tensors\n",
        " *   `forward()` to define the computational graph\n",
        "\n",
        "Let's try a network with one hidden layer, so in terms of parameters we will need:\n",
        " * two weights matrices, but different shapes\n",
        " * two vectors of \"bias weights\" (not connected to inputs)\n",
        "We give these random starting values, and don't forget to tell torch to track gradients through them.\n",
        "\n",
        "I'm going to use $z$ to refer to the input activation level for a neuron. (The usual notation for this is to write this as $\\mathbf{x}\\cdot\\mathbf{w}$ for a single row vector $\\mathbf{x}$, or $\\mathbf{X}\\cdot\\mathbf{w}$ for a whole batch, $\\mathbf{X}$. Oh and must add in the bias as well, $ + \\mathbf{b}$. Perhaps we'll use the `relu` (rectified linear) function as our non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_arlCypJbB"
      },
      "source": [
        "class Net():\n",
        "    def __init__(self):\n",
        "        nHids = 12\n",
        "        self.w1= 0.1 * torch.randn(64,nHids)      # weights to take us from those points to some hidden units\n",
        "        self.b1= 0.1 * torch.randn(nHids)         # one bias, for each hidden unit\n",
        "        self.w2= 0.1 * torch.randn(nHids,10)      # ditto for the second layer, but there's only one output\n",
        "        self.b2= 0.1 * torch.randn(10)            # just one bias, on the sole output\n",
        "\n",
        "        self.w1.requires_grad = True\n",
        "        self.b1.requires_grad = True\n",
        "        self.w2.requires_grad = True\n",
        "        self.b2.requires_grad = True\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = torch.matmul(x,self.w1) + self.b1  # often have to fiddle to get shapes right\n",
        "        h = (z1>0.0) * z1                       # ReLU!\n",
        "        z2 = torch.matmul(h,self.w2) + self.b2  # z2 is the weighted output.\n",
        "        return z2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaYNm9q7jNPF"
      },
      "source": [
        "# quick sanity check\n",
        "net = Net()\n",
        "net.forward(data)[0:3,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXPh1H2xbrU8"
      },
      "source": [
        "### NB: the output looks like it hasn't done a classification yet\n",
        "You might think we should return the output of doing a `softmax` operation at the end of `forward` (to get predicted probabilities of the classes), and then use the negative log Loss for classification (e.g. `torch.nn.NLLLoss`.\n",
        "\n",
        "INSTEAD, here we simply return z2 just as it is, and use [`CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) from the `torch.nn` library instead.\n",
        "\n",
        "_This does the softmax part itself internally_.\n",
        "\n",
        "Doing it this way means we avoid having to include a numerical trick that is needed for doing softmax on large numbers (ask me why!).\n",
        "\n",
        "Incidentally, the technical term for those \"naked\" z2 outputs is \"logits\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ORjFfBTKL8t"
      },
      "source": [
        "lossFn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD([net.w1, net.b1, net.w2, net.b2], lr=0.01)\n",
        "optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now..."
      ],
      "metadata": {
        "id": "_wiOX5-VNpLt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS8PdTOWgbYc"
      },
      "source": [
        "def train_show(network, data, targ, lossFunc, optimiser, epochs):\n",
        "    lossHistory = []  # just to show a plot later...\n",
        "    accuHistory = []\n",
        "\n",
        "    for t in range(epochs):\n",
        "        optimiser.zero_grad()      # Gradients accumulate by default, so don't forget to do this.\n",
        "\n",
        "        y = network.forward(data)  # the forward pass\n",
        "\n",
        "        loss = lossFunc(y,targ)    # recompute the loss\n",
        "        loss.backward()            # runs autograd, to get the gradients needed by optimiser\n",
        "        optimiser.step()           # take a step\n",
        "\n",
        "        # just housekeeping and reporting\n",
        "        accuracy = torch.mean((torch.argmax(y,dim=1) == targ).float())\n",
        "        lossHistory.append(loss.detach().item())\n",
        "        accuHistory.append(accuracy.detach())\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(lossHistory,'r'); plt.title(\"loss\"); plt.xlabel(\"epochs\")\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(accuHistory,'b'); plt.title(\"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yenyCMR-ht3C"
      },
      "source": [
        "# Take it from the top!\n",
        "net = Net()\n",
        "lossFn = torch.nn.CrossEntropyLoss() # see note above\n",
        "\n",
        "optimiser = torch.optim.SGD([net.w1, net.b1, net.w2, net.b2], lr=0.01)\n",
        "\n",
        "train_show(net, data, targ, lossFn, optimiser, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EijrTR_GctOr"
      },
      "source": [
        "# Let's try the same using `torch.nn` library\n",
        "\n",
        "In particular [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html), which has all sorts of \"layers\" prebuilt and optimised.\n",
        "\n",
        "The simplest of these is [linear](https://pytorch.org/docs/stable/generated/torch.nn.functional.linear.html#torch.nn.functional.linear), which does our matrix multiplication (`matmul`) plus bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wyIGR_neqC5"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNEiSY9osgAm"
      },
      "source": [
        "We'll proceed as before, but make our Net a subclass of `torch.nn.Module`.\n",
        "\n",
        "*   one consequence is that, by default, parameters  in `__init__()` get initialised and have `requires_grad` set true\n",
        "*   a `Linear` layer takes care of any biases, as well as the weights\n",
        "*   Notice the reuse of `x` in `forward()` seems gratuitous, but makes adding more layers trivial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nflNi_OEhdi"
      },
      "source": [
        "class OtherNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # here we set up the tensors......\n",
        "        self.layer1 = torch.nn.Linear(64, 12)\n",
        "        self.layer2 = torch.nn.Linear(12, 25)\n",
        "        self.layer3 = torch.nn.Linear(25, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # here we define the (forward) computational graph,\n",
        "        # in terms of the tensors, and elt-wise non-linearities\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isn't that prettier than our first version?"
      ],
      "metadata": {
        "id": "n5GGDEfyZLUy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YoxsCmba7c9"
      },
      "source": [
        "# sanity check\n",
        "othernet = OtherNet()\n",
        "y = othernet.forward(data)\n",
        "lossFn = torch.nn.CrossEntropyLoss()\n",
        "loss = lossFn(y, targ)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l__EH7OxoQjJ"
      },
      "source": [
        "othernet = OtherNet()\n",
        "lossFunction = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimiser = torch.optim.SGD( othernet.parameters(), lr=0.01)\n",
        "# Notice the handy \"net.parameters()\". Before, this was\n",
        "#optimizer = torch.optim.Adam([net.w1, net.b1, net.w2, net.b2], lr=0.01)\n",
        "\n",
        "train_show(othernet, data, targ, lossFunction, optimiser, 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## some representative results?\n",
        "\n",
        "That accuracy looks pretty good (well, on training set!...).\n"
      ],
      "metadata": {
        "id": "e9gefz0mQ4h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naked = othernet.forward(data).detach()\n",
        "naked[0,:]"
      ],
      "metadata": {
        "id": "BXJ7AgEVRBs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm = torch.nn.Softmax(dim=1)  # ie. it's another torch neural net layer in fact\n",
        "output = sm(naked)            # push the \"input\" through this layer...\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "o19inli9Tbkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "id": "lEfcq6uqhTDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = rng.randint(len(data))  # pick a random example to try\n",
        "f, (ax1, ax2) = plt.subplots(1,2)\n",
        "ax1.matshow(data[i].view(8,8)); ax1.axis('off');\n",
        "ax2.bar(range(10), output[i])          # bar chart of the classifier probabilities\n",
        "ax2.plot(targ[i],0,'^r',markersize=20) # red marker for the true class\n",
        "ax2.set_xticks(range(10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Ddu0YVNRbyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyD4JSa0bkNz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}