{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyNQXUE8K86b"
      },
      "source": [
        "#A tutorial introducing PyTorch and an optimiser\n",
        "*Marcus Frean*\n",
        "\n",
        "We build torch code that uses autograd and gradient descent optimisers, for toy search problems in 2D.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKzhx_qWE0yc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z89YqmiIGV1w"
      },
      "source": [
        "Torch gives you Tensors, which are like numpy arrays, but with\n",
        "*   **GPU**/CPU choice via *device_type*, and\n",
        "*   **AutoGrad** via *requires_grad*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM3WUTB6Gx0J"
      },
      "source": [
        "torch.randn(4,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfgFH6PIVcu6"
      },
      "source": [
        "Here we will make something to minimize....\n",
        "\n",
        "Let's call it \"f\" -- in practice this is going to be your loss function.\n",
        "\n",
        "It can be as \"interesting\" as you like, and can call other functions.\n",
        "\n",
        "Here we're going to keep it simple, such as a quadratic bowl, or the Himmelbrau function from the lecture, say.\n",
        "\n",
        "---\n",
        "\n",
        "> But JUST FOR EXAMPLE, it could even involve a forward mapping that *uses* these w to map \"input vectors\" to \"outputs\", and there could be another function that compares those outputs to some \"targets\" and gives a scalar, which f could return..... Just saying!\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXpq65RbHwzX"
      },
      "source": [
        "def f(w):\n",
        "  # EXPECTS A TWO-COLUMN TENSOR (w) whose rows are sample vectors.\n",
        "  #\n",
        "  # Define some interesting surface in a 2-dimensional space.\n",
        "  # See https://en.wikipedia.org/wiki/Test_functions_for_optimization   for lots of ideas.\n",
        "  # These are (x,y) pairs, so we can plot them\n",
        "  # So let's just rename them x and y here, to match the notation on the wikipedia page.\n",
        "  x, y = w[:,0], w[:,1]\n",
        "\n",
        "  # Simple Quadratic bowl:\n",
        "  return x*x + y*y   #ALT:  torch.sum(X*X, 1)  or   torch.mul(X,X), 1) or   X.mul(X)\n",
        "\n",
        "  # Himmelblau:\n",
        "  #return torch.pow(x*x + y -11, 2) + torch.pow(x + y*y -7, 2)\n",
        "\n",
        "  # Bukin6:\n",
        "  #return 100.0 * torch.sqrt(torch.absolute(y-0.01*x*x)) + 0.01*torch.absolute(x + 10)\n",
        "\n",
        "  # Easom = brutal! like a golf putting green.\n",
        "  #return -torch.cos(x) * torch.cos(y) * torch.exp(-((x-np.pi)*(x-np.pi) + (y-np.pi)*(y-np.pi)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.rand(1,2);\n",
        "print(w, \" --> \",f(w))"
      ],
      "metadata": {
        "id": "_vk5fXc0VeeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zmABxJ8VGGH"
      },
      "source": [
        "LIMIT = 6.0\n",
        "NTESTPOINTS = 8\n",
        "winit = LIMIT * (2*torch.rand(NTESTPOINTS,2)-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxGcjxH7VQvl"
      },
      "source": [
        "w = torch.clone(winit)\n",
        "print(w)\n",
        "\n",
        "w.requires_grad = True\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question:** why did we have to say `requires_grad` ?\n",
        "\n",
        "PyTorch will track all operations that involve that tensor. This allows PyTorch to automatically compute gradients for the tensor with respect to a anything (e.g. some loss function we give it)"
      ],
      "metadata": {
        "id": "L2uxMsJ-vLNu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFtMh5QUV0q7"
      },
      "source": [
        "myLoss = f(w)\n",
        "myLoss.backward( torch.ones_like(myLoss) )\n",
        "w.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question:** what did `out.backward` do?\n",
        "\n",
        "The backward function in PyTorch is responsible for **actually computing** those gradients of a function with respect to another tensor, at some point (tensor)."
      ],
      "metadata": {
        "id": "CcJ1kFcgvWzq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPJoHGDFXg9D"
      },
      "source": [
        "# try some flavour of Gradient Descent on `f(w)`\n",
        "\n",
        "At this point we COULD *explicitly* step down the gradient in tiny steps...\n",
        "\n",
        "But instead, we'll use a torch.optimizer to do it for us.\n",
        "\n",
        "(note that generic gradient descent is called `SGD` in torch!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJSQfEgHVY41"
      },
      "source": [
        "opt = torch.optim.SGD([w], lr=0.001, momentum=0.5)  # lr is the learning rate (controls the step size)\n",
        "opt.step()\n",
        "w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that w has now changed.\n",
        "\n",
        "That was one step. Let's try several:"
      ],
      "metadata": {
        "id": "mwPWeyZw4jzG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4309K0XGVxUU"
      },
      "source": [
        "STEPS = 200\n",
        "saved = np.ones((STEPS,len(winit),2)) # this is PURELY for making the pics to follow\n",
        "\n",
        "for t in range(STEPS):\n",
        "  saved[t,:,:] = w.detach().numpy()\n",
        "\n",
        "  opt.zero_grad()\n",
        "  y = f(w)\n",
        "  y.backward(torch.ones_like(y))\n",
        "  opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why did we have to go `w.detach().numpy()` ??\n",
        "\n",
        "\n",
        "\n",
        "> \"detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient. When we don't need a tensor to be traced for the gradient computation, we detach the tensor from the current computational graph. It is most often used when you want to save the loss for logging, or save a Tensor for later inspection but you donâ€™t need gradient information.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "HGZBqlrMvhYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: why did we have to do `opt.zero_grad()` ?"
      ],
      "metadata": {
        "id": "A9WbG998i-HL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now display the trajectories\n",
        "The code here is a little tedious and you need not pay it much attention - we just want to see those trajectories...\n",
        "\n",
        "Obviously this sort of display is only possible because we're working in this tiny toy world with only 2 dimensions.\n",
        "\n",
        "We use `mgrid` as a handy way to make the coordinates of all the points in a \"grid\".\n"
      ],
      "metadata": {
        "id": "7uri_WGIxvYd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnBhiEAkLxXT"
      },
      "source": [
        "grid = np.mgrid[-LIMIT:LIMIT:0.1,  -LIMIT:LIMIT:0.1]\n",
        "n, m  = grid.shape[1], grid.shape[2]\n",
        "xs, ys = grid[0], grid[1]\n",
        "colors = ['b','g','r','c','m','y']\n",
        "\n",
        "gridInputs = torch.from_numpy(grid.reshape(2,-1).T)\n",
        "truesurf = f(gridInputs).numpy()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.contour(xs,ys, truesurf.reshape(n,m), levels=50, alpha=.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "for i in range(len(winit)):\n",
        "  c = colors[i % len(colors)]\n",
        "  plt.plot(saved[0,i,0],saved[0,i,1],'s',color=c,markersize=10)\n",
        "  plt.plot(saved[:,i,0],saved[:,i,1],'-',color=c)#'gray')\n",
        "  plt.plot(saved[:,i,0],saved[:,i,1],'o',color=c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-EnIXVh0Uyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}