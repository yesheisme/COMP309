{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oywaxxFTGg1x"
      },
      "source": [
        "\n",
        "# Tutorial 7a: Features\n",
        "\n",
        "COMP309-2024-T2\n",
        "\n",
        "Marcus Frean\n",
        "\n",
        "*with thanks to Baligh Al-Helali (PhD from VUW, 2021)*\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXMT-z2grpXQ"
      },
      "source": [
        "## transformers\n",
        "These tasks are done using **transformers**\n",
        "(not to be confused with the neural network architecture of the same name).\n",
        "\n",
        "SciKit-Learn's \"transformer\" is used for this, the main methods being:\n",
        "- transformer.fit()\n",
        "- transformer.transform()\n",
        "- transformer.fit_transform()\n",
        "\n",
        "Note that the analysis and fitting(training) is based only on the train dataset.\n",
        "After that, the learned transformations are applied to the test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq7u4kOBWz31"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1UFX8zXEaQ"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTxGTqSaXH_9"
      },
      "outputs": [],
      "source": [
        "df=sns.load_dataset('iris')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "bQCq-Yc8yBLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRQupXjFZGrF"
      },
      "outputs": [],
      "source": [
        "# Seperate the target variable\n",
        "X=df[df.columns[1:-1]]   # read \"-1\" as \"the last one\"\n",
        "y=df[df.columns[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mNkeK9nhc2b"
      },
      "outputs": [],
      "source": [
        "# split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofur5jUwQ6zw"
      },
      "outputs": [],
      "source": [
        "#Check before standardization\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1yRSfmQ4cL"
      },
      "source": [
        "## **PCA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGFUI-WFXkC4"
      },
      "source": [
        "### Standardize the Data\n",
        "\n",
        "PCA is affected by scale: you should give each of the features in your data a similar scale (mean = 0 and variance = 1) before applying PCA.\n",
        "We will use `StandardScaler` to standardize our datasetâ€™s features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-agz97xHBYuW"
      },
      "outputs": [],
      "source": [
        "#Now lets apply 1-1 \"StandardScaler\" transformer\n",
        "#1) import the module\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#2) define the model\n",
        "scaler=StandardScaler()\n",
        "\n",
        "#3) fit the model\n",
        "scaler.fit(X_train)\n",
        "\n",
        "#4) transform the data\n",
        "X_train_ss = scaler.transform(X_train)\n",
        "\n",
        "# note 3 and 4 could be combined like this:\n",
        "X_train_ss = scaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run PCA"
      ],
      "metadata": {
        "id": "uIx8qBcFrkDx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aLCO2d1Z6BK"
      },
      "outputs": [],
      "source": [
        "#Now lets perfrom pca\n",
        "#Steps are similar to the scale transformer\n",
        "#1) import the module\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#2) define the model\n",
        "pca = PCA(n_components=2)   # n_components means the pca transformation constructs this many features\n",
        "\n",
        "#3) fit the model\n",
        "pca.fit(X_train_ss)\n",
        "\n",
        "#4) transform the data\n",
        "pca_train = pca.transform(X_train_ss)\n",
        "\n",
        "# Again, 3 and 4 could be combined\n",
        "pca_train = pca.fit_transform(X_train_ss)\n",
        "\n",
        "# print the output, which is a matrix of only two features\n",
        "pca_train[:10,:]\n",
        "# ALT: plt.scatter(pca_train[:,0],pca_train[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qJpnXigjaP7"
      },
      "source": [
        "### Visualising the results\n",
        "\n",
        "possible if it's 2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwXbpkpNbgd4"
      },
      "outputs": [],
      "source": [
        "#format and visualise the transformed training data\n",
        "df_pca_train = pd.DataFrame(data = pca_train, columns = ['pc1', 'pc2'])\n",
        "df_pca_train['species']=y_train\n",
        "sns.scatterplot(x='pc1', y='pc2', hue=df_pca_train['species'], data=df_pca_train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUvOfwMmz-j"
      },
      "source": [
        "### Transform the test data\n",
        "\n",
        "note: Here we only apply the learned transformers to transform the test data, so there's no \"fitting\" here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuID7NCfnLMI"
      },
      "outputs": [],
      "source": [
        "#1- First apply the scaler that has been built based on the training data to scale the test data\n",
        "X_test_ss = scaler.transform(X_test)\n",
        "\n",
        "#2- Second apply the pca transformation that has been built based on the training data to transfer the scaled test data\n",
        "pca_test = pca.transform(X_test_ss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXqTEokilwrw"
      },
      "source": [
        "###Classification\n",
        "\n",
        "Let's try using the original features only to do classification, and then see if things get better with the new features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRurs6JBvN8D"
      },
      "outputs": [],
      "source": [
        "# Performing classification based on the original data\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(random_state=0)\n",
        "#classifier=SVC()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "score = accuracy_score(classifier.predict(X_test), y_test)\n",
        "print('Accuracy before transformation  = {:.2f}'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vmRv7NzprPt"
      },
      "outputs": [],
      "source": [
        "# Performing classification using the pca-based transformed data\n",
        "classifier.fit(pca_train, y_train)\n",
        "score = accuracy_score(classifier.predict(pca_test), y_test)\n",
        "print('Accuracy after PCA transformation  = {:.2f}'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0uRfarir_Td"
      },
      "source": [
        "##ICA\n",
        "\n",
        "Steps are very similar to the scaler and the PCA transformeres.\n",
        "\n",
        "There are several ICA approaches in fact. We will use sklearn's `Fast ICA` algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbgXGaSQqFkf"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "ica = FastICA(n_components=2)\n",
        "\n",
        "ica_train = ica.fit_transform(X_train_ss)  # nb. we already did the scaling, above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOhnnHa9tdAP"
      },
      "outputs": [],
      "source": [
        "# Visualisation\n",
        "df_ica_train = pd.DataFrame(data = ica_train, columns = ['ic1', 'ic2'])\n",
        "df_ica_train['species']=y_train\n",
        "sns.scatterplot(x='ic1', y='ic2', hue=df_ica_train['species'], data=df_ica_train);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vLBf1vqtkYz"
      },
      "outputs": [],
      "source": [
        "# Performing classification using the ica-based transformed data\n",
        "# Transform test data using ica\n",
        "classifier.fit(ica_train, y_train)\n",
        "ica_test = ica.transform(X_test_ss)\n",
        "score = accuracy_score(classifier.predict(ica_test), y_test)\n",
        "print('Accuracy after ICA transformation  = {:.2f}'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_DbUogtv7Hj"
      },
      "source": [
        "## GP transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tszb1Us8wsDD"
      },
      "outputs": [],
      "source": [
        "# Might need to install the package for genetic programming (gp)\n",
        "!pip install gplearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALyW6vF-vggh"
      },
      "outputs": [],
      "source": [
        "#Since this package does not work when the target variable is string, an encoder is used to convert it\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "label_encoded = le.fit_transform(y_train)\n",
        "label_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgMixJDZx_3Q"
      },
      "outputs": [],
      "source": [
        "#Now lets apply genetic programming.\n",
        "#Steps are similar to the scale, pca, and ica transformers\n",
        "from gplearn.genetic import SymbolicTransformer\n",
        "gp = SymbolicTransformer(n_components=2)\n",
        "gp.fit(X_train_ss, label_encoded)\n",
        "gp_train = gp.transform(X_train_ss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOIoCd6Ty8Wq"
      },
      "outputs": [],
      "source": [
        "# Visualisation using the gp-based transformed data\n",
        "df_gp_train = pd.DataFrame(data = gp_train, columns = ['gp1', 'gp2'])\n",
        "df_gp_train['species']=y_train\n",
        "sns.scatterplot(x='gp1', y='gp2', hue=df_gp_train['species'], data=df_gp_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRta9_wezKF4"
      },
      "outputs": [],
      "source": [
        "# Transform test data using gp\n",
        "# Then, performing classification using the ica-based transformed data\n",
        "gp_test = gp.transform(X_test_ss)\n",
        "df_gp_test = pd.DataFrame(data = gp_test, columns = ['gp1', 'gp2'])\n",
        "df_gp_test['species']=y_test\n",
        "classifier.fit(gp_train, y_train)\n",
        "accuracy_score(classifier.predict(gp_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj23hsNUiESM"
      },
      "source": [
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}